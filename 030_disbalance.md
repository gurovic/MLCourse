```python
# –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤

# !pip install pandas numpy scikit-learn imbalanced-learn tensorflow
```

---

## üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å (–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã)

### 1.1 –ü–æ–Ω–∏–º–∞–Ω–∏–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
```python
from sklearn.datasets import make_classification

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
X, y = make_classification(
    n_samples=1000, 
    weights=[0.95, 0.05],  # 95% negative class
    random_state=42
)

# –ê–Ω–∞–ª–∏–∑
print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:", {0: (y == 0).sum(), 1: (y == 1).sum()})
```

### 1.2 –°–ª—É—á–∞–π–Ω–æ–µ –ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
```python
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# –ü–µ—Ä–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
oversampler = RandomOverSampler(sampling_strategy=0.5, random_state=42)
X_over, y_over = oversampler.fit_resample(X, y)

# –ù–µ–¥–æc—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state=42)
X_under, y_under = undersampler.fit_resample(X, y)
```

### 1.3 –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
```python
from sklearn.linear_model import LogisticRegression

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ
model = LogisticRegression(class_weight='balanced')

# –†—É—á–Ω–æ–µ –∑–∞–¥–∞–Ω–∏–µ –≤–µ—Å–æ–≤
weights = {0: 1, 1: 10}  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
model = LogisticRegression(class_weight=weights)
```

---

## üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å (SMOTE –∏ –∞–Ω—Å–∞–º–±–ª–∏)

### 2.1 SMOTE (Synthetic Minority Oversampling)
```python
from imblearn.over_sampling import SMOTE

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
smote = SMOTE(
    sampling_strategy=0.3,
    k_neighbors=5,
    random_state=42
)
X_smote, y_smote = smote.fit_resample(X, y)
```

### 2.2 –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã
```python
from imblearn.ensemble import BalancedRandomForestClassifier

# –ú–æ–¥–µ–ª—å —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π
brf = BalancedRandomForestClassifier(
    n_estimators=100,
    sampling_strategy=0.5,
    replacement=True,
    random_state=42
)
brf.fit(X, y)
```

### 2.3 –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
```python
from sklearn.metrics import classification_report

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ F1 –≤–º–µ—Å—Ç–æ accuracy
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))
```

---

## üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å (–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏)

### 3.1 –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ (GAN)
```python
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
import numpy as np

# –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
def build_generator():
    input = Input(shape=(10,))
    x = Dense(128, activation='relu')(input)
    output = Dense(X.shape[1], activation='tanh')(x)
    return Model(input, output)

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GAN –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
# (–ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞)
```

### 3.2 –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
```python
class DynamicWeightedLoss:
    """–ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –≤–µ—Å–∞–º–∏"""
    def __init__(self, alpha=0.5):
        self.alpha = alpha  # –ö–æ–Ω—Ç—Ä–æ–ª—å –±–∞–ª–∞–Ω—Å–∞
        
    def __call__(self, y_true, y_pred):
        pos_mask = (y_true == 1)
        pos_weight = (1 - self.alpha) / K.sum(pos_mask)
        neg_weight = self.alpha / K.sum(1 - pos_mask)
        weights = pos_weight * pos_mask + neg_weight * (1 - pos_mask)
        return K.mean(weights * K.binary_crossentropy(y_true, y_pred))
```

### 3.3 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
```python
from sklearn.metrics import precision_recall_curve

# –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
best_threshold = thresholds[np.argmax(f1_scores)]

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞
y_pred_custom = (y_probs >= best_threshold).astype(int)
```

---

## üìä –ß–µ–∫–ª–∏—Å—Ç –ø–æ —É—Ä–æ–≤–Ω—è–º

| –£—Ä–æ–≤–µ–Ω—å | –ù–∞–≤—ã–∫–∏ |
|---------|--------|
| üü¢ | RandomOverSampler, class_weight, –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è |
| üü° | SMOTE, BalancedRandomForest, –º–µ—Ç—Ä–∏–∫–∏ F1/ROC-AUC |
| üî¥ | GAN-—Å–∏–Ω—Ç–µ–∑, –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ |

---

## ‚ö†Ô∏è –ê–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã
### –î–ª—è –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π:
- **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ accuracy –∫–∞–∫ –º–µ—Ç—Ä–∏–∫–∏** –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–°–ª–µ–ø–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE** –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–∏—Ä–æ–¥—ã –¥–∞–Ω–Ω—ã—Ö
- **–ü–æ–ª–Ω–æ–µ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞** (–º–æ–∂–µ—Ç —É—Ö—É–¥—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ)

### üî¥ –≠–∫—Å–ø–µ—Ä—Ç—ã:
- **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö** (—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
- **–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ costs-sensitive –∞–Ω–∞–ª–∏–∑–∞** (—Ä–∞–∑–Ω–∞—è —Ü–µ–Ω–∞ –æ—à–∏–±–æ–∫)

---

## üöÄ –°–æ–≤–µ—Ç—ã
1. **–ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π:**
```python
from yellowbrick.classifier import ClassBalance
visualizer = ClassBalance(labels=['Class 0', 'Class 1'])
visualizer.fit(y_train, y_test)
visualizer.show()
```

2. **–ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ –º–µ—Ç–æ–¥—ã:**
```python
from imblearn.pipeline import Pipeline

pipeline = Pipeline([
    ('smote', SMOTE(sampling_strategy=0.3)),
    ('undersample', RandomUnderSampler(sampling_strategy=0.5)),
    ('model', LogisticRegression())
])
```

3. **–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ —á–µ—Ä–µ–∑ –º–∞—Ç—Ä–∏—Ü—É:**
```python
from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true')
```

---

## üìà –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä: –ö—Ä–µ–¥–∏—Ç–Ω–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ
```python
# üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–¥—Ö–æ–¥
from imblearn.ensemble import EasyEnsembleClassifier

# EasyEnsemble: –∞–Ω—Å–∞–º–±–ª—å –∏–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥–≤—ã–±–æ—Ä–æ–∫
eec = EasyEnsembleClassifier(
    n_estimators=10,
    sampling_strategy=0.5,
    random_state=42
)
eec.fit(X_train, y_train)

# üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é CTGAN
from ctgan import CTGANSynthesizer

ctgan = CTGANSynthesizer()
ctgan.fit(df_minority, ['fraud'])  # –û–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–º –∫–ª–∞—Å—Å–µ
synthetic_data = ctgan.sample(1000)
```

---

## ÔøΩ –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è

### üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
**–ó–∞–¥–∞—á–∞ 1:** –ü—Ä–∏–º–µ–Ω–∏—Ç–µ RandomUnderSampler –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ (fraud_detection.csv). –°—Ä–∞–≤–Ω–∏—Ç–µ F1-score –¥–æ/–ø–æ—Å–ª–µ.

### üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å
**–ó–∞–¥–∞—á–∞ 2:** –ò—Å–ø–æ–ª—å–∑—É—è SMOTE, —Å–±–∞–ª–∞–Ω—Å–∏—Ä—É–π—Ç–µ –∫–ª–∞—Å—Å—ã –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∏–∞–≥–Ω–æ–∑–æ–≤. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ ROC-–∫—Ä–∏–≤—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π LogisticRegression –∏ BalancedRandomForest.

### üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
**–ó–∞–¥–∞—á–∞ 3:** –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–∞—Å—Ç–æ–º–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –≤ PyTorch, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º –±–∞—Ç—á–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º –∫–ª–∞—Å—Å–æ–≤ 1:100.

---

```python
# –ü—Ä–∏–º–µ—Ä —Ä–µ—à–µ–Ω–∏—è –¥–ª—è üü¢ –ó–∞–¥–∞—á–∏ 1
import pandas as pd
from sklearn.model_selection import train_test_split

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv('fraud_detection.csv')
X = df.drop('is_fraud', axis=1)
y = df['is_fraud']

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Undersampling
undersampler = RandomUnderSampler(sampling_strategy=0.5)
X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)

# –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞
model = LogisticRegression(max_iter=1000)
model.fit(X_resampled, y_resampled)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

---

## üìå –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
–ö–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏:
1. **–ù–µ –≤—Å–µ–≥–¥–∞ –Ω—É–∂–Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å—ã** ‚Äî –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–∞–¥–∞—á–∏.
2. **–°–æ—á–µ—Ç–∞–π—Ç–µ –º–µ—Ç–æ–¥—ã** (–Ω–∞–ø—Ä–∏–º–µ—Ä, SMOTE + Undersampling).
3. **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏** ‚Äî Precision/Recall Tradeoff.
4. **–£—á–∏—Ç—ã–≤–∞–π—Ç–µ —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—à–∏–±–æ–∫** (–ª–æ–∂–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ vs. –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ).
