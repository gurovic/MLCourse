### Практическое задание: Влияние масштабирования на kNN  
**Цель:** Убедиться, что масштабирование признаков реально влияет на качество kNN, и понять, почему это происходит.

---

#### Что будем делать:
1. **Подготовим два датасета:**
   - "Луны" (`make_moons`) - 2D данные в форме полумесяцев
   - "Ирисы" (`load_iris`) - классический набор с 4 признаками

2. **Проверим kNN в трех вариантах:**
   - Без масштабирования
   - С `StandardScaler` (приводит к среднему=0, std=1)
   - С `MinMaxScaler` (сжимает в диапазон [0,1])

3. **Сравним результаты:**
   - Посчитаем точность (accuracy)
   - Для "Лун" построим графики решающих границ

---

#### Что увидим (ожидаемые результаты):

**Для "Лун":**  
- **Без масштабирования:**  
Границы выглядят "колючими" и неестественными. Точность ~80-85%.  
Почему? kNN путается из-за разного масштаба осей.

- **Со масштабированием:**  
Границы становятся плавными, повторяют форму полумесяцев.  
Точность подскакивает до 90-95%!  
Разница между StandardScaler и MinMaxScaler почти незаметна.

**Для "Ирисов":**  
- **Без масштабирования:**  
Точность ~85-90%. Модель переоценивает признаки с большими числами (например, длина чашелистика).

- **Со масштабированием:**  
Точность вырастает до 93-97%!  
MinMaxScaler иногда чуть лучше StandardScaler, так как все признаки становятся равнозначными.

---

#### Почему это важно:
1. **kNN основан на расстояниях** - если один признак измеряется в тысячах, а другой в долях, модель будет игнорировать "мелкие" признаки
2. **Масштабирование = честные правила игры** - все признаки получают равный вес
3. **Разные алгоритмы - разная чувствительность**:
   - kNN, SVM, кластеризация - критичны к масштабу
   - Деревья, бустинг - обычно нет

**Вывод:** Всегда масштабируйте данные перед kNN! Это 5 минут работы, которые могут поднять точность на 10-15%.
