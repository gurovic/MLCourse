К линейным моделям машинного обучения (ML) относят те, где **прогнозируемое значение (целевая переменная) является линейной комбинацией входных признаков (features)**. Это означает, что модель строит **линейную зависимость** между независимыми переменными (`X`) и зависимой переменной (`y`).

**Основные линейные модели:**

1.  **Линейная регрессия (Linear Regression):**
    *   **Почему линейная?** Самая базовая модель. Предсказывает непрерывную величину `y` как взвешенную сумму входных признаков: `y = β₀ + β₁*x₁ + β₂*x₂ + ... + βₙ*xₙ + ε`, где `β` - коэффициенты (веса), `ε` - ошибка. Графически представляет собой прямую линию (1D), плоскость (2D) или гиперплоскость (nD).

2.  **Логистическая регрессия (Logistic Regression):**
    *   **Почему линейная?** Хотя она предсказывает *вероятность* принадлежности к классу (бинарная или мультиклассовая классификация), сама **граница решения** является линейной. Логит-функция (`log(p/(1-p))`) линейно зависит от признаков: `log(p/(1-p)) = β₀ + β₁*x₁ + ... + βₙ*xₙ`. Сама вероятность `p` преобразуется нелинейно (сигмоидой), но решение о классе (где `p = 0.5`) лежит на линейной гиперплоскости.

3.  **Метод опорных векторов с линейным ядром (Linear SVM):**
    *   **Почему линейная?** Цель SVM - найти гиперплоскость, максимально разделяющую классы (в случае классификации) или гиперплоскость, вокруг которой есть трубка с минимальной ошибкой (в случае регрессии). Когда используется **линейное ядро (`kernel='linear'`)**, SVM ищет именно **линейную** разделяющую гиперплоскость (или гиперплоскость регрессии). Нелинейные ядра (RBF, полиномиальное) делают модель нелинейной.

4.  **Линейный дискриминантный анализ (LDA) и Фишера (FDA):**
    *   **Почему линейные?** Эти методы классификации проецируют данные на новое пространство (меньшей размерности), максимизирующее разделимость классов. Полученные **дискриминантные функции являются линейными комбинациями исходных признаков**. Граница решения между классами - линейная гиперплоскость в этом новом пространстве (или исходном, если проекция не используется).

5.  **Перцептрон (Perceptron):**
    *   **Почему линейная?** Простейшая архитектура нейронной сети. Вычисляет взвешенную сумму входов и применяет ступенчатую функцию активации (или просто порог). Его **решающая граница** - линейная гиперплоскость (`w·x + b = 0`).

6.  **Лассо (Lasso) и Гребневая регрессия (Ridge Regression):**
    *   **Почему линейные?** Это **регуляризованные** версии линейной регрессии. Они добавляют штраф к функции потерь (L1-норма весов для Лассо, L2-норма для Риджа), чтобы уменьшить переобучение и/или выполнить отбор признаков (Lasso). **Ключевой момент:** Сама **основная модель предсказания `y` остается линейной комбинацией признаков**, как и в обычной линейной регрессии. Регуляризация влияет только на *способ нахождения коэффициентов* `β`, но не меняет линейную формулу предсказания.

**Обобщенно: Почему эти модели считаются линейными?**

1.  **Линейная комбинация признаков:** Прогноз `ŷ` (или логит в случае логистической регрессии) формируется как `ŷ = β₀ + β₁*x₁ + β₂*x₂ + ... + βₙ*xₙ`. Это линейное уравнение.
2.  **Линейность по параметрам:** Модель линейна **относительно своих параметров (коэффициентов `β`)**. Это критически важно. Даже если признаки (`X`) преобразованы нелинейно (например, `x²`, `log(x)`, `x1*x2`), **если итоговый прогноз `ŷ` по-прежнему является *линейной* функцией от этих *преобразованных* признаков (`φ(X)`), модель остается линейной** (это иногда называют *линейной моделью по базисным функциям*). Например, полиномиальная регрессия `ŷ = β₀ + β₁*x + β₂*x²` линейна по параметрам `β`, хотя и моделирует квадратичную зависимость от исходного признака `x`.
3.  **Линейные границы решений:** В задачах классификации линейные модели создают решающие границы в виде прямых линий, плоскостей или гиперплоскостей в пространстве признаков (или преобразованном линейном пространстве).

**Что НЕ является линейной моделью (для контраста):**

*   **Деревья решений (Decision Trees, Random Forest, Gradient Boosting Machines):** Делают предсказания на основе иерархии нелинейных условий (`if-else`).
*   **Метод k-ближайших соседей (k-NN):** Предсказание основано на среднем значении или моде ближайших точек, нет линейной формулы.
*   **Нейронные сети с нелинейными функциями активации (ReLU, sigmoid, tanh) и/или более чем одним скрытым слоем:** Способны аппроксимировать сложные нелинейные функции.
*   **SVM с нелинейными ядрами (RBF, полиномиальное):** Ядро неявно преобразует данные в пространство высокой размерности, где разделяющая граница линейна, но в исходном пространстве она нелинейна.
*   **Наивный Байес (GaussianNB):** Основан на теореме Байеса и предположении о независимости признаков, прогнозирует вероятности, не являясь линейной комбинацией признаков.

Таким образом, линейность модели определяется **формой зависимости прогноза от входных признаков (через параметры)**, а не обязательно линейностью этой зависимости от *исходных* сырых данных.
