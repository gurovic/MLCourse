
# –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è 

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# !pip install scikit-learn
```

---

## üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å (–û—Å–Ω–æ–≤—ã)

### 1.1 –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞
**–£—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:**  
$y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$  
**–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (MSE):**  
$MSE = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2$

### 1.2 –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
```python
# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 5, 8])

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = LinearRegression()
model.fit(X, y)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
y_pred = model.predict([[5]])
print(f"–ü—Ä–æ–≥–Ω–æ–∑: {y_pred[0]:.2f}")  # –û–∂–∏–¥–∞–µ–º ~9.5
```

### 1.3 –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
```python
mse = mean_squared_error(y, model.predict(X))
print(f"MSE: {mse:.2f}")  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å < 1.0
```

---

## üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å (–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫)

### 2.1 –†—É—á–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è MSE
```python
def compute_mse(w0, w1, X, y):
    predictions = w0 + w1 * X
    return np.mean((y - predictions) ** 2)
```

### 2.2 –ê–ª–≥–æ—Ä–∏—Ç–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞
```python
def gradient_descent(X, y, lr=0.01, epochs=1000):
    w0, w1 = 0, 0
    n = len(X)
    
    for _ in range(epochs):
        y_pred = w0 + w1 * X
        grad_w0 = (-2/n) * np.sum(y - y_pred)
        grad_w1 = (-2/n) * np.sum(X * (y - y_pred))
        w0 -= lr * grad_w0
        w1 -= lr * grad_w1
    
    return w0, w1

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
w0, w1 = gradient_descent(X.flatten(), y)
print(f"–£—Ä–∞–≤–Ω–µ–Ω–∏–µ: y = {w0:.2f} + {w1:.2f}x")
```

### 2.3 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
```python
plt.plot(X, y, 'bo', label='–î–∞–Ω–Ω—ã–µ')
plt.plot(X, w0 + w1*X, 'r-', label='–ü—Ä–æ–≥–Ω–æ–∑')
plt.title('–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º')
plt.legend()
plt.show()
```

---

## üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å (–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏)

### 3.1 –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
```python
def stochastic_gd(X, y, lr=0.01, epochs=100):
    w0, w1 = 0, 0
    n = len(X)
    
    for _ in range(epochs):
        for i in range(n):
            y_pred = w0 + w1 * X[i]
            grad_w0 = -2 * (y[i] - y_pred)
            grad_w1 = -2 * X[i] * (y[i] - y_pred)
            w0 -= lr * grad_w0
            w1 -= lr * grad_w1
    
    return w0, w1
```

### 3.2 –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Ridge)
```python
# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –∫ MSE
def ridge_mse(w0, w1, X, y, alpha=0.1):
    mse = compute_mse(w0, w1, X, y)
    return mse + alpha * (w0**2 + w1**2)
```

### 3.3 –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã (Adam)
```python
# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Adam Optimizer
def adam_optimizer(X, y, lr=0.01, beta1=0.9, beta2=0.999, epochs=1000):
    w0, w1 = 0, 0
    m_w0, v_w0 = 0, 0
    m_w1, v_w1 = 0, 0
    eps = 1e-8
    
    for t in range(1, epochs+1):
        y_pred = w0 + w1 * X
        grad_w0 = -2 * np.mean(y - y_pred)
        grad_w1 = -2 * np.mean(X * (y - y_pred))
        
        m_w0 = beta1 * m_w0 + (1 - beta1) * grad_w0
        m_w1 = beta1 * m_w1 + (1 - beta1) * grad_w1
        
        v_w0 = beta2 * v_w0 + (1 - beta2) * grad_w0**2
        v_w1 = beta2 * v_w1 + (1 - beta2) * grad_w1**2
        
        m_w0_hat = m_w0 / (1 - beta1**t)
        m_w1_hat = m_w1 / (1 - beta1**t)
        
        v_w0_hat = v_w0 / (1 - beta2**t)
        v_w1_hat = v_w1 / (1 - beta2**t)
        
        w0 -= lr * m_w0_hat / (np.sqrt(v_w0_hat) + eps)
        w1 -= lr * m_w1_hat / (np.sqrt(v_w1_hat) + eps)
    
    return w0, w1
```

---

## üìä –ß–µ–∫–ª–∏—Å—Ç –ø–æ —É—Ä–æ–≤–Ω—è–º

| –£—Ä–æ–≤–µ–Ω—å | –ù–∞–≤—ã–∫–∏ |
|---------|--------|
| üü¢ | –ü–æ–Ω–∏–º–∞–Ω–∏–µ MSE, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LinearRegression |
| üü° | –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è |
| üî¥ | –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã |

---

## ‚ö†Ô∏è –ê–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã
1. **–ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö** –ø–µ—Ä–µ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º
2. **–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π learning rate** (—Ä–∞—Å—Ö–æ–¥–∏–º–æ—Å—Ç—å)
3. **–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è** –±–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
4. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ MSE** –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å –≤—ã–±—Ä–æ—Å–∞–º–∏

---

## üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å–æ–≤–µ—Ç—ã
1. **–ü–æ–¥–±–æ—Ä learning rate:**
```python
for lr in [0.1, 0.01, 0.001]:
    w0, w1 = gradient_descent(X, y, lr=lr)
    mse = compute_mse(w0, w1, X, y)
    print(f"LR: {lr}, MSE: {mse:.2f}")
```

2. **–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞:**
```python
best_mse = float('inf')
for epoch in range(epochs):
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
    current_mse = compute_mse(w0, w1, X, y)
    if current_mse > best_mse:
        break
    best_mse = current_mse
```

3. **–ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:**
```python
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
```

---

## üìå –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è

### üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö: X=[[1], [2], [3]], y=[1, 3, 5]. –ü—Ä–µ–¥—Å–∫–∞–∂–∏—Ç–µ y –¥–ª—è X=4.
2. –†–∞—Å—Å—á–∏—Ç–∞–π—Ç–µ MSE –º–µ–∂–¥—É –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ y=[2,4,6] –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ [1.9, 3.8, 6.1].

### üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å 2 —Ñ–∏—á–∞–º–∏.
2. –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ MSE –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.

### üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –†–µ–∞–ª–∏–∑—É–π—Ç–µ ElasticNet —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (L1+L2).
2. –°—Ä–∞–≤–Ω–∏—Ç–µ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ Adam vs SGD.

---

```python
# –ü—Ä–∏–º–µ—Ä —Ä–µ—à–µ–Ω–∏—è üü¢ –ó–∞–¥–∞–Ω–∏—è 1
X = np.array([[1], [2], [3]])
y = np.array([1, 3, 5])
model = LinearRegression()
model.fit(X, y)
print(f"–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è 4: {model.predict([[4]])[0]:.2f}")  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å ~7.0
```

---

## üìå –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
1. **MSE —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º** ‚Äì –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ MAE –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
2. **–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ —Ç—Ä–µ–±—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è** (StandardScaler)
3. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è** –ø–æ–º–æ–≥–∞–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º
4. **–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã** (Adam) —á–∞—Å—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ vanilla GD

–ü–æ–º–Ω–∏—Ç–µ: –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è ‚Äî –±–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è. –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ:
- –õ–∏–Ω–µ–π–Ω–æ—Å—Ç—å —Å–≤—è–∑–∏ (—á–µ—Ä–µ–∑ scatter plots)
- –ì–æ–º–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –æ—Å—Ç–∞—Ç–∫–æ–≤
- –ù–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫
