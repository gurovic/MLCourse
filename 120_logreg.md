
# –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

# !pip install scikit-learn
```

---

## üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å (–û—Å–Ω–æ–≤—ã)

### 1.1 –°–∏–≥–º–æ–∏–¥–∞ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
**–§–æ—Ä–º—É–ª–∞ —Å–∏–≥–º–æ–∏–¥—ã:**  
$\sigma(z) = \frac{1}{1 + e^{-z}}$  
–≥–¥–µ $z = w_0 + w_1x_1 + ... + w_nx_n$

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# –ü—Ä–∏–º–µ—Ä
z = np.linspace(-7, 7, 100)
plt.plot(z, sigmoid(z))
plt.title('–°–∏–≥–º–æ–∏–¥–∞')
plt.xlabel('z')
plt.ylabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
plt.grid(True)
plt.show()
```

### 1.2 –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
```python
# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
X = np.array([[1.5], [2.0], [3.0], [4.0], [5.0]])
y = np.array([0, 0, 1, 1, 1])

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = LogisticRegression()
model.fit(X, y)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
probabilities = model.predict_proba(X)[:, 1]
print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {probabilities}")
```

### 1.3 –ü–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
```python
# –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–æ—Ä–æ–≥ = 0.5
predictions = model.predict(X)
print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {predictions}")

# –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞
custom_predictions = (probabilities > 0.3).astype(int)
print(f"–ö–∞—Å—Ç–æ–º–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {custom_predictions}")
```

---

## üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å (–†–µ–∞–ª–∏–∑–∞—Ü–∏—è)

### 2.1 –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (Log Loss)
**–§–æ—Ä–º—É–ª–∞:**  
$LogLoss = -\frac{1}{N}\sum_{i=1}^{N}[y_i\log(p_i) + (1-y_i)\log(1-p_i)]$

```python
def log_loss(y_true, y_pred_proba):
    epsilon = 1e-15  # –î–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    y_pred_proba = np.clip(y_pred_proba, epsilon, 1-epsilon)
    return -np.mean(y_true * np.log(y_pred_proba) + (1-y_true) * np.log(1-y_pred_proba))
```

### 2.2 –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
```python
def logistic_gd(X, y, lr=0.01, epochs=1000):
    w = np.zeros(X.shape[1])
    b = 0
    n = len(X)
    losses = []
    
    for epoch in range(epochs):
        z = np.dot(X, w) + b
        p = sigmoid(z)
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
        dw = (1/n) * np.dot(X.T, (p - y))
        db = (1/n) * np.sum(p - y)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        w -= lr * dw
        b -= lr * db
        
        # –õ–æ—Å—Å
        loss = log_loss(y, p)
        losses.append(loss)
        
    return w, b, losses

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
X_with_bias = np.c_[np.ones(len(X)), X]  # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü –¥–ª—è bias
w, b, losses = logistic_gd(X_with_bias, y)
```

### 2.3 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
```python
plt.plot(losses)
plt.title('–°—Ö–æ–¥–∏–º–æ—Å—Ç—å Log Loss')
plt.xlabel('–ò—Ç–µ—Ä–∞—Ü–∏—è')
plt.ylabel('Log Loss')
plt.show()
```

---

## üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å (–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏)

### 3.1 –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
```python
# L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
def logistic_gd_reg(X, y, lr=0.01, epochs=1000, lambda_=0.1):
    w = np.zeros(X.shape[1])
    b = 0
    n = len(X)
    
    for epoch in range(epochs):
        z = np.dot(X, w) + b
        p = sigmoid(z)
        
        dw = (1/n) * (np.dot(X.T, (p - y)) + (lambda_/n) * w
        db = (1/n) * np.sum(p - y)
        
        w -= lr * dw
        b -= lr * db
        
    return w, b

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å sklearn
model = LogisticRegression(penalty='l2', C=1/lambda_)
```

### 3.2 –ü–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–∞ –ø–æ ROC-–∫—Ä–∏–≤–æ–π
```python
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_true, y_probs)
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]
```

### 3.3 –ú—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
```python
# –°—Ç—Ä–∞—Ç–µ–≥–∏–∏: one-vs-rest (OvR) –∏–ª–∏ multinomial
model = LogisticRegression(multi_class='multinomial', solver='lbfgs')

# –ü—Ä–∏–º–µ—Ä –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤
X_multi = np.array([[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]])
y_multi = np.array([0, 0, 1, 1, 2, 2])

model.fit(X_multi, y_multi)
print("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:", model.predict([[2.5]]))
```

---

## üìä –ß–µ–∫–ª–∏—Å—Ç –ø–æ —É—Ä–æ–≤–Ω—è–º

| –£—Ä–æ–≤–µ–Ω—å | –ù–∞–≤—ã–∫–∏ |
|---------|--------|
| üü¢ | –°–∏–≥–º–æ–∏–¥–∞, –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, predict_proba |
| üü° | Log Loss, –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ |
| üî¥ | –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –ø–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–∞, –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å |

---

## ‚ö†Ô∏è –ê–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã
1. **–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤** (—Ç–æ—á–Ω–æ—Å—Ç—å 99% –ø—Ä–∏ 99% –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞)
2. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏** –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
3. **–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è** –ø—Ä–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º —Å–ø—É—Å–∫–µ
4. **–í—ã–±–æ—Ä –ø–æ—Ä–æ–≥–∞ 0.5 –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞** ROC-–∫—Ä–∏–≤–æ–π

---

## üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å–æ–≤–µ—Ç—ã
1. **–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π:**
```python
from sklearn.calibration import CalibratedClassifierCV
calibrated = CalibratedClassifierCV(model, cv=3, method='sigmoid')
calibrated.fit(X_train, y_train)
```

2. **–ê–Ω–∞–ª–∏–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤:**
```python
coef = model.coef_[0]
features = X.columns
plt.barh(features, coef)
plt.title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
```

3. **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —á–µ—Ä–µ–∑ odds ratio:**
```python
odds_ratio = np.exp(coef)
print(f"–®–∞–Ω—Å—ã —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç—Å—è –≤ {odds_ratio[0]:.2f} —Ä–∞–∑ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–∞ 1 –µ–¥–∏–Ω–∏—Ü—É")
```

---

## üìå –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è

### üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ –¥–∞–Ω–Ω—ã—Ö: X=[[1], [2], [3], [4]], y=[0, 0, 1, 1].
2. –ü—Ä–µ–¥—Å–∫–∞–∂–∏—Ç–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è X=2.5.

### üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –¥–ª—è —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö.
2. –°—Ä–∞–≤–Ω–∏—Ç–µ Log Loss —Å sklearn.

### üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –î–æ–±–∞–≤—å—Ç–µ L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ —Å–≤–æ—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é.
2. –ù–∞–π–¥–∏—Ç–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ ROC-AUC.

---

```python
# –ü—Ä–∏–º–µ—Ä —Ä–µ—à–µ–Ω–∏—è üü¢ –ó–∞–¥–∞–Ω–∏—è 1
X = np.array([[1], [2], [3], [4]])
y = np.array([0, 0, 1, 1])
model = LogisticRegression()
model.fit(X, y)
print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è 2.5: {model.predict_proba([[2.5]])[0][1]:.2f}")
```

---

## üìå –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
1. **–°–∏–≥–º–æ–∏–¥–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ª–∏–Ω–µ–π–Ω—É—é –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å**
2. **Log Loss –ª—É—á—à–µ MSE –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏**
3. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞** –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
4. **–ü–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å** –ø–æ–¥ –∑–∞–¥–∞—á—É

–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è ‚Äî –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ "—Ä–µ–≥—Ä–µ—Å—Å–∏—è". –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç:
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ odds ratio
- –†–∞–±–æ—Ç–∞—Ç—å —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏, –∞ –Ω–µ "—á–µ—Ä–Ω–æ-–±–µ–ª—ã–º–∏" –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
- –õ–µ–≥–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
