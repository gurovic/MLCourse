
# –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å ‚Äì –±—É—Ç—Å—Ç—Ä—ç–ø, –∞–≥—Ä–µ–≥–∞—Ü–∏—è

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.datasets import load_breast_cancer, make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error

# !pip install scikit-learn
```

---

## üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å (–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏)

### 1.1 –ß—Ç–æ —Ç–∞–∫–æ–µ —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å?
**–ê–Ω—Å–∞–º–±–ª–µ–≤—ã–π –º–µ—Ç–æ–¥:** –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π  
**–î–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞:**
1. **–ë—É—Ç—Å—Ç—Ä—ç–ø (Bootstrap):** –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º
2. **–ê–≥—Ä–µ–≥–∞—Ü–∏—è (Bagging):** –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–µ—Ä–µ–≤—å–µ–≤

```python
# –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
print(f"Accuracy: {accuracy_score(y_test, rf.predict(X_test)):.3f}")
```

### 1.2 –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ –æ–¥–∏–Ω–æ—á–Ω—ã–º –¥–µ—Ä–µ–≤–æ–º
- –£–º–µ–Ω—å—à–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
- –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º
- –û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

### 1.3 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ä–µ–≤—å–µ–≤ –ª–µ—Å–∞
```python
# –ü–µ—Ä–≤—ã–µ 3 –¥–µ—Ä–µ–≤–∞
plt.figure(figsize=(15, 10))
for i in range(3):
    plt.subplot(1, 3, i+1)
    tree = rf.estimators_[i]
    plot_tree(tree, feature_names=load_breast_cancer().feature_names, 
              filled=True, max_depth=2)
plt.tight_layout()
plt.show()
```

---

## üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å (–†–µ–∞–ª–∏–∑–∞—Ü–∏—è)

### 2.1 –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –±—ç–≥–≥–∏–Ω–≥–∞
```python
class SimpleBagging:
    def __init__(self, base_estimator, n_estimators=10):
        self.estimators = [clone(base_estimator) for _ in range(n_estimators)]
        
    def fit(self, X, y):
        for estimator in self.estimators:
            # –ë—É—Ç—Å—Ç—Ä—ç–ø –≤—ã–±–æ—Ä–∫–∞
            indices = np.random.choice(len(X), size=len(X), replace=True)
            estimator.fit(X[indices], y[indices])
            
    def predict(self, X):
        predictions = np.array([estimator.predict(X) for estimator in self.estimators])
        return np.mean(predictions, axis=0) > 0.5  # –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
```

### 2.2 –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
```python
# –ù–∞ –æ—Å–Ω–æ–≤–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–µ—á–∏—Å—Ç–æ—Ç—ã
importances = rf.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
forest_importances = pd.Series(importances, index=load_breast_cancer().feature_names)
forest_importances.sort_values().plot(kind='barh', xerr=std, figsize=(10, 8))
plt.title("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
plt.xlabel("–°—Ä–µ–¥–Ω–µ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–µ—á–∏—Å—Ç–æ—Ç—ã")
plt.show()
```

### 2.3 –ê–Ω–∞–ª–∏–∑ Out-of-Bag –æ—à–∏–±–∫–∏
```python
rf = RandomForestClassifier(oob_score=True, n_estimators=200)
rf.fit(X_train, y_train)
print(f"OOB Score: {rf.oob_score_:.3f}")

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–æ–π
y_pred = rf.predict(X_test)
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.3f}")
```

---

## üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å (–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏)

### 3.1 Extremely Randomized Trees
```python
from sklearn.ensemble import ExtraTreesClassifier

# –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –ø–æ—Ä–æ–≥–æ–≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
et = ExtraTreesClassifier(n_estimators=100, random_state=42)
et.fit(X_train, y_train)
print(f"ExtraTrees Accuracy: {accuracy_score(y_test, et.predict(X_test)):.3f}")
```

### 3.2 –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_features': ['sqrt', 'log2', None],
    'max_depth': [5, 10, None]
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
```

### 3.3 –†–µ–≥—Ä–µ—Å—Å–∏—è –∏ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
```python
# –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1)
rf_reg = RandomForestRegressor(n_estimators=100)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
rf_reg.fit(X_reg, y_reg)
predictions = np.array([tree.predict(X_reg) for tree in rf_reg.estimators_])
mean_pred = np.mean(predictions, axis=0)
std_pred = np.std(predictions, axis=0)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(10, 6))
plt.errorbar(y_reg[:50], mean_pred[:50], yerr=1.96*std_pred[:50], fmt='o', alpha=0.7)
plt.plot([min(y_reg), max(y_reg)], [min(y_reg), max(y_reg)], 'r--')
plt.title("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º")
plt.xlabel("–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
plt.ylabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
plt.show()
```

### 3.4 GPU-—É—Å–∫–æ—Ä–µ–Ω–∏–µ
```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ cuML –¥–ª—è GPU
# !pip install cuml

from cuml.ensemble import RandomForestClassifier as cuRFC

gpu_rf = cuRFC(n_estimators=100)
gpu_rf.fit(X_train, y_train)
print(f"GPU Accuracy: {accuracy_score(y_test, gpu_rf.predict(X_test)):.3f}")
```

---

## üìä –ß–µ–∫–ª–∏—Å—Ç –ø–æ —É—Ä–æ–≤–Ω—è–º

| –£—Ä–æ–≤–µ–Ω—å | –ù–∞–≤—ã–∫–∏ |
|---------|--------|
| üü¢ | –ü–æ–Ω–∏–º–∞–Ω–∏–µ –±—É—Ç—Å—Ç—Ä—ç–ø–∞ –∏ –±—ç–≥–≥–∏–Ω–≥–∞, –±–∞–∑–æ–≤–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ |
| üü° | –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –±—ç–≥–≥–∏–Ω–≥–∞, –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏, OOB-–æ—à–∏–±–∫–∞ |
| üî¥ | ExtraTrees, –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã, GPU-—É—Å–∫–æ—Ä–µ–Ω–∏–µ |

---

## ‚ö†Ô∏è –ê–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã
1. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–µ–∑ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** (–æ—Å–æ–±–µ–Ω–Ω–æ max_depth)
2. **–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ OOB-–æ—Ü–µ–Ω–∫–∏** –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏
3. **–°–ª–µ–ø–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏
4. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–º –¥–∞–Ω–Ω—ã–º** –±–µ–∑ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

---

## üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å–æ–≤–µ—Ç—ã
1. **–ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏:**
```python
# –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
corr_matrix = pd.DataFrame(X_train).corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]
filtered_importances = importances[~np.isin(feature_names, to_drop)]
```

2. **–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:**
```python
from sklearn.ensemble import VotingClassifier
from sklearn.svm import SVC

ensemble = VotingClassifier([
    ('rf', RandomForestClassifier(n_estimators=100)),
    ('svm', SVC(probability=True))
], voting='soft')
```

3. **–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:**
```python
# –ß–µ—Ä–µ–∑ warm_start
rf = RandomForestClassifier(warm_start=True, n_estimators=50)
rf.fit(X_train[:100], y_train[:100])
rf.n_estimators += 50
rf.fit(X_train, y_train)  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ
```

---

## üìå –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è

### üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –û–±—É—á–∏—Ç–µ —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ Iris, —Å—Ä–∞–≤–Ω–∏—Ç–µ accuracy —Å –æ–¥–∏–Ω–æ—á–Ω—ã–º –¥–µ—Ä–µ–≤–æ–º.
2. –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

### üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –ø—Ä–æ—Å—Ç–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –±—ç–≥–≥–∏–Ω–≥–∞ –¥–ª—è 10 –¥–µ—Ä–µ–≤—å–µ–≤.
2. –°—Ä–∞–≤–Ω–∏—Ç–µ OOB-–æ—Ü–µ–Ω–∫—É —Å —Ç–µ—Å—Ç–æ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.

### üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏.
2. –°—Ä–∞–≤–Ω–∏—Ç–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ CPU –∏ GPU –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ 500k —Å—Ç—Ä–æ–∫.

---

```python
# –ü—Ä–∏–º–µ—Ä —Ä–µ—à–µ–Ω–∏—è üü¢ –ó–∞–¥–∞–Ω–∏—è 1
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# –û–¥–∏–Ω–æ—á–Ω–æ–µ –¥–µ—Ä–µ–≤–æ
tree = DecisionTreeClassifier(max_depth=3)
tree.fit(X_train, y_train)
tree_acc = accuracy_score(y_test, tree.predict(X_test))

# –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å
rf = RandomForestClassifier(n_estimators=50)
rf.fit(X_train, y_train)
rf_acc = accuracy_score(y_test, rf.predict(X_test))

print(f"Tree Accuracy: {tree_acc:.3f}, RF Accuracy: {rf_acc:.3f}")
```

---

## üìå –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
1. **–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–µ—Ä–µ–≤—å–µ–≤** ‚Üí —Å–∏–ª–∞ –ª–µ—Å–∞ (—Ä–∞–∑–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ + —Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
2. **OOB-–æ—Ü–µ–Ω–∫–∞** –∫–∞–∫ –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–æ–≥ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
3. **–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** - –ø–æ–±–æ—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç –æ–±—É—á–µ–Ω–∏—è
4. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–µ—Ä–µ–≤—å–µ–≤

–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å:
- –†–∞–±–æ—Ç–∞–µ—Ç "–∏–∑ –∫–æ—Ä–æ–±–∫–∏" –±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
- –•–æ—Ä–æ—à –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –î–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- –õ–µ–≥–∫–æ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–µ—Ç—Å—è

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ–≥–æ –∫–∞–∫ baseline –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –∫ –±—É—Å—Ç–∏–Ω–≥–∞–º!
