# **Feature Engineering –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö**  

## **–í–≤–µ–¥–µ–Ω–∏–µ –≤ Feature Engineering –¥–ª—è —Ç–µ–∫—Å—Ç–∞**  
–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Äî –∫–ª—é—á–µ–≤–æ–π —ç—Ç–∞–ø –≤ NLP. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º:  
- üìä **–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã** (—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)  
- üîç **–õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏** (–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è, —Å–∏–Ω—Ç–∞–∫—Å–∏—Å)  
- üß© **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏** –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞  

**–û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏:**  
- –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π ML  
- –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏  
- –í—ã–¥–µ–ª–µ–Ω–∏–µ —Å–º—ã—Å–ª–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤  

---

## **üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å (–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)**  

### **1.1 –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏**  
```python
def text_statistics(text):
    return {
        'num_chars': len(text),
        'num_words': len(text.split()),
        'num_unique_words': len(set(text.split())),
        'avg_word_length': sum(len(word) for word in text.split()) / len(text.split()),
        'num_sentences': text.count('.') + text.count('!') + text.count('?')
    }

text = "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –≠—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ!"
stats = text_statistics(text)
# {'num_chars': 27, 'num_words': 4, ...}
```

### **1.2 –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ N-–≥—Ä–∞–º–º**  
```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = ["–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"]
vectorizer = CountVectorizer(ngram_range=(2, 2))  # –±–∏–≥—Ä–∞–º–º—ã
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names_out())  # ['–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–æ–±—É—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç']
```

### **1.3 –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å—Ç–∏–ª—è —Ç–µ–∫—Å—Ç–∞**  
```python
def style_features(text):
    return {
        'punct_density': sum(1 for char in text if char in '.,!?;:') / len(text),
        'capitals_ratio': sum(1 for char in text if char.isupper()) / len(text),
        'exclamations': text.count('!')
    }
```

---

## **üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å (–õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)**  

### **2.1 –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞)**  
```python
from pymorphy2 import MorphAnalyzer

morph = MorphAnalyzer()

def morph_features(word):
    parse = morph.parse(word)[0]
    return {
        'POS': parse.tag.POS,  # —á–∞—Å—Ç—å —Ä–µ—á–∏
        'case': parse.tag.case,
        'tense': parse.tag.tense if parse.tag.tense else None
    }

morph_features("–±–µ–≥—É")  # {'POS': 'VERB', 'case': None, 'tense': 'pres'}
```

### **2.2 –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å**  
```python
import spacy

nlp = spacy.load("ru_core_news_sm")

def syntax_complexity(text):
    doc = nlp(text)
    return {
        'avg_deps': sum(len(token.dep_) for token in doc) / len(doc),
        'num_clauses': sum(1 for sent in doc.sents for token in sent if token.dep_ == 'conj')
    }
```

### **2.3 –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ FastText**  
```python
import fasttext
import fasttext.util

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä—É—Å—Å–∫–æ–π –º–æ–¥–µ–ª–∏
ft = fasttext.load_model('cc.ru.300.bin')

def get_text_vector(text):
    words = text.split()
    return sum(ft.get_word_vector(word) for word in words) / len(words)  # —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
```

---

## **üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å (–ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)**  

### **3.1 –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (LDA)**  
```python
from sklearn.decomposition import LatentDirichletAllocation

lda = LatentDirichletAllocation(n_components=5)
lda.fit(tfidf_matrix)  # –º–∞—Ç—Ä–∏—Ü–∞ TF-IDF

def get_topic_features(text):
    tfidf = vectorizer.transform([text])
    return lda.transform(tfidf)[0]  # —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–µ–º–∞–º
```

### **3.2 –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏**  
```python
def domain_specific_features(text):
    return {
        'has_legal_terms': int(any(word in text for word in ['–¥–æ–≥–æ–≤–æ—Ä', '—Å—Ç–æ—Ä–æ–Ω–∞'])),
        'has_tech_terms': int(any(word in text for word in ['–∞–ª–≥–æ—Ä–∏—Ç–º', '–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å']))
    }
```

### **3.3 –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**  
```python
from sklearn.pipeline import FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin

class TextStatsTransformer(BaseEstimator, TransformerMixin):
    def transform(self, texts):
        return [list(text_statistics(text).values()) for text in texts]

pipeline = FeatureUnion([
    ('tfidf', TfidfVectorizer()),
    ('stats', TextStatsTransformer())
])
```

---

## **üìå –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è**  

### **üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å**  
1. –î–ª—è —Ç–µ–∫—Å—Ç–∞ "–ù–õ–ü - —ç—Ç–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" –≤—ã—á–∏—Å–ª–∏—Ç–µ:  
   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Å–ª–æ–≤  
   - –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è  

### **üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å**  
1. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –º–∞—Ç—Ä–∏—Ü—É –±–∏–≥—Ä–∞–º–º –¥–ª—è –∫–æ—Ä–ø—É—Å–∞ —Ä—É—Å—Å–∫–∏—Ö –ø–æ—Å–ª–æ–≤–∏—Ü  
2. –ò–∑–≤–ª–µ–∫–∏—Ç–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è 10 —Ä—É—Å—Å–∫–∏—Ö –≥–ª–∞–≥–æ–ª–æ–≤  

### **üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å**  
1. –°–æ–∑–¥–∞–π—Ç–µ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ "–Ω–∞—É—á–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞" –Ω–∞ –æ—Å–Ω–æ–≤–µ:  
   - –î–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π  
   - –ß–∞—Å—Ç–æ—Ç—ã —Ç–µ—Ä–º–∏–Ω–æ–≤  
   - –ß–∞—Å—Ç–∏ —Ä–µ—á–∏  

---

## **üí° –ó–∞–∫–ª—é—á–µ–Ω–∏–µ**  
**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã Feature Engineering –¥–ª—è —Ç–µ–∫—Å—Ç–∞:**  
1. **–ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ –ø–æ–¥—Ö–æ–¥—ã** (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ + –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞ + —Ç–µ–º–∞—Ç–∏–∫–∞)  
2. **–£—á–∏—Ç—ã–≤–∞–π—Ç–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫—É —è–∑—ã–∫–∞** (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)  
3. **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –∫–æ–º–ø–æ–∑–∏—Ü–∏–µ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**  
4. **–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–π—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å** (–æ—Ç–±–∏—Ä–∞–π—Ç–µ –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)  

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã:**  
- –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤: —É–ø–æ—Ä –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏ —Å—Ç–∏–ª–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏  
- –î–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: LDA + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞  
- –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: TF-IDF + —ç–º–±–µ–¥–¥–∏–Ω–≥–∏  

> **"–•–æ—Ä–æ—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–∞—Å—Ç–æ –≤–∞–∂–Ω–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π!"**  

**–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞:**  
- `pymorphy2` ‚Äî –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑  
- `natasha` ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π  
- `ru_core_news_sm` (spacy) ‚Äî —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–æ—Ä
