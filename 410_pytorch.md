# **PyTorch Basics: –¢–µ–Ω–∑–æ—Ä—ã –∏ Autograd**  

## **–í–≤–µ–¥–µ–Ω–∏–µ –≤ PyTorch**  
PyTorch ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –≥–∏–±–∫–æ—Å—Ç—å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å.  
**–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**  
- ‚ö° **–¢–µ–Ω–∑–æ—Ä—ã** ‚Äî –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –º–∞—Å—Å–∏–≤—ã —Å GPU-—É—Å–∫–æ—Ä–µ–Ω–∏–µ–º  
- üîÑ **Autograd** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤  
- üß© **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π** ‚Äî –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–µ—Ç–∏ –Ω–∞ –ª–µ—Ç—É  

**–ó–∞—á–µ–º —É—á–∏—Ç—å PyTorch?**  
- –ò–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–π Python-like API  
- –®–∏—Ä–æ–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö (90% —Å—Ç–∞—Ç–µ–π –Ω–∞ NeurIPS)  
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è (TorchServe, TorchScript)  

---

## **üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å: –†–∞–±–æ—Ç–∞ —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏**  

### **1.1 –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤**  
```python
import torch

# –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–∞
tensor_a = torch.tensor([1, 2, 3])  # –≤–µ–∫—Ç–æ—Ä [1, 2, 3]

# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã
zeros = torch.zeros(2, 3)       # –º–∞—Ç—Ä–∏—Ü–∞ 2x3 –∏–∑ –Ω—É–ª–µ–π  
rand_matrix = torch.rand(3, 3)  # —Å–ª—É—á–∞–π–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è 0-1

# –ù–∞ GPU (—É—Å–∫–æ—Ä–µ–Ω–∏–µ ~10-100x)
if torch.cuda.is_available():
    tensor_gpu = rand_matrix.cuda()
```

### **1.2 –û–ø–µ—Ä–∞—Ü–∏–∏ —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏**  
```python
a = torch.tensor([1.0, 2.0], requires_grad=True)
b = torch.tensor([3.0, 4.0], requires_grad=True)

# –ë–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
c = a + b            # –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —Å–ª–æ–∂–µ–Ω–∏–µ [4.0, 6.0]
d = torch.dot(a, b)  # —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ 1*3 + 2*4 = 11.0

# –ú–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
mat1 = torch.randn(2, 3)
mat2 = torch.randn(3, 2)
mat_mul = torch.mm(mat1, mat2)  # –º–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
```

### **1.3 –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã**  
```python
tensor = torch.arange(12).reshape(3, 4)  # –º–∞—Ç—Ä–∏—Ü–∞ 3x4

# –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
row = tensor[1]        # –≤—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞ [4, 5, 6, 7]
element = tensor[0, 2] # —ç–ª–µ–º–µ–Ω—Ç (1,3) ‚Üí 2

# –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã
flattened = tensor.flatten()  # –≤–µ–∫—Ç–æ—Ä –∏–∑ 12 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
transposed = tensor.T         # —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ 4x3
```

---

## **üü° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å: Autograd –≤ –¥–µ–π—Å—Ç–≤–∏–∏**  

### **2.1 –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ**  
```python
# –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã —Å —Ñ–ª–∞–≥–æ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(1.5, requires_grad=True)
b = torch.tensor(0.7, requires_grad=True)

# –í—ã—á–∏—Å–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é
y = w * x + b  # –ª–∏–Ω–µ–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è

# –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
y.backward()  # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ

print(f"dy/dw = {w.grad}")  # 2.0 (x)
print(f"dy/db = {b.grad}")  # 1.0
```

### **2.2 –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π**  
```python
x = torch.tensor([1.0, 2.0], requires_grad=True)
z = torch.prod(x)  # z = x1 * x2 = 2.0

z.backward()  # dz/dx1 = x2 = 2.0, dz/dx2 = x1 = 1.0
print(x.grad)  # [2.0, 1.0]
```

### **2.3 –ö–æ–Ω—Ç—Ä–æ–ª—å –ø–æ—Ç–æ–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π**  
```python
# –û—Ç–∫–ª—é—á–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
with torch.no_grad():
    y = x * 2  # –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–µ –±—É–¥—É—Ç –∑–∞–ø–∏—Å–∞–Ω—ã –≤ –≥—Ä–∞—Ñ

# –†—É—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
model.zero_grad()  # –æ–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–µ—Ä–µ–¥ –Ω–æ–≤—ã–º backward()
```

---

## **üî¥ –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞—Ñ—ã**  

### **3.1 –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å autograd**  
```python
class CustomReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)
    
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
x = torch.randn(4, requires_grad=True)
y = CustomReLU.apply(x)
y.backward(torch.ones_like(y))
```

### **3.2 –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞**  
```python
x = torch.tensor(3.0, requires_grad=True)
y = x**2 + 2*x

# –ü–µ—Ä–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]

# –í—Ç–æ—Ä–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]
print(f"d¬≤y/dx¬≤ = {d2y_dx2.item()}")  # 2.0
```

### **3.3 –û—Ç–ª–∞–¥–∫–∞ –≥—Ä–∞—Ñ–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π**  
```python
# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ (—Ç—Ä–µ–±—É–µ—Ç—Å—è torchviz)
from torchviz import make_dot

x = torch.tensor(2.0, requires_grad=True)
y = x**3 + torch.sin(x)
make_dot(y).render("graph", format="png")  # —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ –≤ PNG
```

---

## **üöÄ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä: –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è**  

```python
import matplotlib.pyplot as plt

# –î–∞–Ω–Ω—ã–µ: y = 1.5*x + 0.8 + —à—É–º
x = torch.linspace(0, 1, 100)
y_true = 1.5 * x + 0.8
y_noisy = y_true + 0.1 * torch.randn_like(x)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
w = torch.tensor(0.0, requires_grad=True)
b = torch.tensor(0.0, requires_grad=True)

# –û–±—É—á–µ–Ω–∏–µ
optimizer = torch.optim.SGD([w, b], lr=0.1)
for epoch in range(100):
    y_pred = w * x + b
    loss = torch.mean((y_pred - y_noisy)**2)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 10 == 0:
        print(f"Epoch {epoch}: w={w.item():.3f}, b={b.item():.3f}")

# –†–µ–∑—É–ª—å—Ç–∞—Ç
plt.scatter(x, y_noisy, label='–î–∞–Ω–Ω—ã–µ')
plt.plot(x, y_pred.detach(), 'r-', label='–ü—Ä–æ–≥–Ω–æ–∑')
plt.legend()
plt.show()
```

---

## **üíé –ó–∞–∫–ª—é—á–µ–Ω–∏–µ**  
**–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ PyTorch:**  
1. **–¢–µ–Ω–∑–æ—Ä—ã** ‚Äî –æ—Å–Ω–æ–≤–∞ –≤—Å–µ—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç GPU-—É—Å–∫–æ—Ä–µ–Ω–∏–µ  
2. **Autograd** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏  
3. **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ** ‚Äî –≥–∏–±–∫–æ—Å—Ç—å –≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä  

**–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏:**  
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `.detach()` –¥–ª—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤  
- –†–µ–≥—É–ª—è—Ä–Ω–æ –≤—ã–∑—ã–≤–∞–π—Ç–µ `.zero_grad()` –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏  
- –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –≥—Ä–∞—Ñ—ã –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π  

> **"PyTorch –¥–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –∏ –∏–Ω–∂–µ–Ω–µ—Ä–∞–º —Å–≤–æ–±–æ–¥—É —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞, —Å–æ—á–µ—Ç–∞—è –ø—Ä–æ—Å—Ç–æ—Ç—É Python —Å –º–æ—â—å—é GPU."**  

**–î–∞–ª—å–Ω–µ–π—à–µ–µ –∏–∑—É—á–µ–Ω–∏–µ:**  
- [–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã PyTorch](https://pytorch.org/tutorials/)  
- [Deep Learning —Å PyTorch](https://practicaldeeplearning.ai/)  
- [–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∫—É—Ä—Å Kaggle](https://www.kaggle.com/learn/pytorch)
